{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tumor image segmentation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import json\n",
    "import os\n",
    "from typing import Any\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import seaborn.objects as so\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "savedir = 'analysis/plots'\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-hoc CRC only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crc = pd.read_csv('out/polyps/crc.csv')\n",
    "df_crc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot FNR, FPR, and λ as a function of α for the post-hoc CRC method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crc_melt = pd.melt(df_crc, id_vars=('seed', 'alpha'), value_vars=('fnr', 'fpr', 'lambda'))\n",
    "df_crc_melt = df_crc_melt.rename({'variable': 'metric'}, axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), tight_layout=True)\n",
    "\n",
    "(\n",
    "    so.Plot(df_crc_melt, x='alpha', y='value')\n",
    "    .add(so.Dot(), so.Dodge(), so.Jitter(.3))\n",
    "    .facet('metric')\n",
    "    .scale(x=so.Continuous().tick(at=df_crc_melt['alpha'].unique().tolist()))\n",
    "    .scale(color=so.Nominal())\n",
    "    .share(y=False)\n",
    "    .on(fig).plot()\n",
    ")\n",
    "\n",
    "axs = fig.get_axes()\n",
    "axs[2].set(yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conformal Risk Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1]\n",
    "seeds = range(10)\n",
    "lrs = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "cols = ('seed', 'alpha', 'lr', 'test_fpr', 'test_fnr', 'best_epoch', 'val_fpr', 'val_lam', 'ckpt_path')\n",
    "\n",
    "rows = []\n",
    "for a, s, lr in itertools.product(alphas, seeds, lrs):\n",
    "    basename = f'a{a:.2f}_lr{lr:.2g}_s{s}'\n",
    "    try:\n",
    "        with open(f'out/polyps/e2ecrc/{basename}.json') as f:\n",
    "            result = json.load(f)\n",
    "        assert result['seed'] == s\n",
    "        assert result['alpha'] == a\n",
    "        assert result['lr'] == lr\n",
    "        result['ckpt_path'] = f'out/polyps/e2ecrc/{basename}.pth'\n",
    "        rows.append({k: result[k] for k in cols})\n",
    "    except FileNotFoundError:\n",
    "        print(f'File not found: {basename}.json')\n",
    "        continue\n",
    "\n",
    "df_e2e = pd.DataFrame(rows)\n",
    "df_e2e.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot FNR, FPR, and λ as a function of α for Conformal Risk Training. Color by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e2e_melt = pd.melt(df_e2e, id_vars=('seed', 'alpha', 'lr'), value_vars=('test_fnr', 'test_fpr', 'val_lam'))\n",
    "df_e2e_melt = df_e2e_melt.rename({'variable': 'metric'}, axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), tight_layout=True)\n",
    "\n",
    "(\n",
    "    so.Plot(df_e2e_melt, x='alpha', y='value', color='lr')\n",
    "    .add(so.Dot(), so.Dodge(), so.Jitter(.3))\n",
    "    .facet('metric')\n",
    "    .scale(x=so.Continuous().tick(at=df_e2e_melt['alpha'].unique().tolist()))\n",
    "    .scale(color=so.Nominal())\n",
    "    .share(y=False)\n",
    "    .on(fig).plot()\n",
    ")\n",
    "\n",
    "axs = fig.get_axes()\n",
    "axs[2].set(yscale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output columns:\n",
    "# seed, alpha, lambda, fnr, fpr\n",
    "best_hps = df_e2e.groupby(['alpha', 'seed'])['val_fpr'].idxmin().values.tolist()\n",
    "df_e2e_best = df_e2e.loc[best_hps].reset_index().set_index(['alpha', 'seed'])\n",
    "\n",
    "df_e2e_best = df_e2e_best.rename(columns={'test_fpr': 'fpr', 'test_fnr': 'fnr', 'val_lam': 'lambda'})\n",
    "df_e2e_best = df_e2e_best.reset_index()[['seed', 'alpha', 'lambda', 'fnr', 'fpr', 'ckpt_path']]\n",
    "\n",
    "df_e2e_best.sort_values(['seed', 'alpha']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy\n",
    "\n",
    "Training pre-trained model with more epochs on cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = [0.01, 0.05, 0.1]\n",
    "seeds = range(10)\n",
    "lrs = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n",
    "\n",
    "rows = []\n",
    "for s, lr in itertools.product(seeds, lrs):\n",
    "    basename = f'lr{lr:.2g}_s{s}'\n",
    "    try:\n",
    "        with open(f'out/polyps/trainbase/{basename}.json') as f:\n",
    "            result = json.load(f)\n",
    "        assert result['seed'] == s\n",
    "        assert result['lr'] == lr\n",
    "        assert set(alphas).issubset(result['alphas'])\n",
    "        for i, a in enumerate(result['alphas']):\n",
    "            rows.append({\n",
    "                'alpha': a,\n",
    "                'seed': s,\n",
    "                'lr': lr,\n",
    "                'val_loss': result['val_loss'],\n",
    "                'val_lam': result['val_lams'][i],\n",
    "                'test_fpr': result['test_fprs'][i],\n",
    "                'test_fnr': result['test_fnrs'][i],\n",
    "                'ckpt_path': f'out/polyps/trainbase/{basename}.pt'\n",
    "            })\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f'File not found: {basename}.json')\n",
    "        continue\n",
    "\n",
    "df_trainbase = pd.DataFrame(rows)\n",
    "df_trainbase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot FNR, FPR, and λ as a function of α for \"cross-entropy\" method. Color by learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trainbase_melt = pd.melt(df_trainbase, id_vars=('seed', 'alpha', 'lr'), value_vars=('test_fnr', 'test_fpr', 'val_lam'))\n",
    "df_trainbase_melt = df_trainbase_melt.rename({'variable': 'metric'}, axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), tight_layout=True)\n",
    "\n",
    "(\n",
    "    so.Plot(df_trainbase_melt, x='alpha', y='value', color='lr')\n",
    "    .add(so.Dot(), so.Dodge(), so.Jitter(.3))\n",
    "    .facet('metric')\n",
    "    .scale(x=so.Continuous().tick(at=df_trainbase_melt['alpha'].unique().tolist()))\n",
    "    .scale(color=so.Nominal())\n",
    "    .share(y=False)\n",
    "    .on(fig).plot()\n",
    ")\n",
    "\n",
    "axs = fig.get_axes()\n",
    "axs[2].set(yscale='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output columns:\n",
    "# seed, alpha, lambda, fnr, fpr\n",
    "best_hps = df_trainbase.groupby(['alpha', 'seed'])['val_loss'].idxmin().values.tolist()\n",
    "df_trainbase_best = df_trainbase.loc[best_hps].reset_index().set_index(['alpha', 'seed'])\n",
    "\n",
    "df_trainbase_best = df_trainbase_best.rename(columns={'test_fpr': 'fpr', 'test_fnr': 'fnr', 'val_lam': 'lambda'})\n",
    "df_trainbase_best = df_trainbase_best.reset_index()[['seed', 'alpha', 'lambda', 'fnr', 'fpr', 'ckpt_path']]\n",
    "\n",
    "df_trainbase_best.sort_values(['seed', 'alpha']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_crc['model'] = 'post-hoc CRC'\n",
    "df_trainbase_best['model'] = 'cross-entropy'\n",
    "df_e2e_best['model'] = 'conformal risk training'\n",
    "df = pd.concat([df_crc, df_e2e_best, df_trainbase_best], axis=0)\n",
    "\n",
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate relative FPR reduction of conformal risk training vs best baseline,\n",
    "# where best baseline is min(cross-entropy, post-hoc CRC)\n",
    "fprs = df.set_index(['seed', 'alpha', 'model']).unstack('model')['fpr']\n",
    "fprs = fprs.loc[fprs['conformal risk training'].notna()]\n",
    "\n",
    "fprs['best baseline'] = np.minimum(fprs['cross-entropy'], fprs['post-hoc CRC'])\n",
    "\n",
    "rel_fpr = (fprs['best baseline'] - fprs['conformal risk training']) / fprs['best baseline']\n",
    "rel_fpr.groupby('alpha').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.melt(df, id_vars=('seed', 'alpha', 'model'), value_vars=('fnr', 'fpr', 'lambda'))\n",
    "df = df.rename({'variable': 'metric'}, axis=1)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(['alpha', 'metric', 'model'])['value'].agg(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=df[df['alpha'] <= 0.1], x='alpha', y='value',\n",
    "    hue='model', hue_order=('post-hoc CRC', 'cross-entropy', 'conformal risk training'),\n",
    "    col='metric', kind='box', sharey=False,\n",
    "    height=2.85\n",
    ")\n",
    "\n",
    "axes = g.axes.flatten()\n",
    "\n",
    "axes[2].set(yscale='log')\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for ax in axes:\n",
    "    ax.set(xlabel=r'$\\alpha$')\n",
    "    for i, label in enumerate(ax.get_xticklabels()):\n",
    "        if i > 0:\n",
    "            ax.axvline(i - 0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "axes[0].set(ylabel=None, yticks=[0, 0.05, 0.1])\n",
    "axes[0].set(title='False Negative Rate')\n",
    "axes[1].set(title='False Positive Rate')\n",
    "axes[2].set(title=r'$\\lambda$')\n",
    "\n",
    "g.tight_layout(pad=0, w_pad=1.08)\n",
    "g.figure.savefig(os.path.join(savedir, 'polyps.pdf'), pad_inches=0)\n",
    "g.figure.savefig(os.path.join(savedir, 'polyps.png'), pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=df[(df['alpha'] <= 0.1) & (df['metric'] != 'lambda')],\n",
    "    x='alpha', y='value',\n",
    "    hue='model', hue_order=('post-hoc CRC', 'cross-entropy', 'conformal risk training'),\n",
    "    col='metric', kind='box', sharey=False,\n",
    "    height=2.85\n",
    ")\n",
    "\n",
    "axes = g.axes.flatten()\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for ax in axes:\n",
    "    ax.set(xlabel=r'$\\alpha$')\n",
    "    for i, label in enumerate(ax.get_xticklabels()):\n",
    "        if i > 0:\n",
    "            ax.axvline(i - 0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "axes[0].set(ylabel=None, yticks=[0, 0.05, 0.1])\n",
    "axes[0].set(title='False Negative Rate')\n",
    "axes[1].set(title='False Positive Rate')\n",
    "\n",
    "g.tight_layout(pad=0, w_pad=1.08)\n",
    "# g.figure.savefig(os.path.join(savedir, 'polyps.pdf'), pad_inches=0)\n",
    "# g.figure.savefig(os.path.join(savedir, 'polyps.png'), pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda plot\n",
    "fig, ax = plt.subplots(figsize=(6, 2.8), tight_layout=True)\n",
    "sns.boxplot(\n",
    "    data=df[(df['alpha'] <= 0.1) & (df['metric'] == 'lambda')],\n",
    "    x='alpha', y='value', hue='model',\n",
    "    hue_order=('post-hoc CRC', 'cross-entropy', 'conformal risk training'),\n",
    ")\n",
    "\n",
    "# place legend outside the plot on the right-hand side, vertically centered\n",
    "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5), title='model', facecolor='white', edgecolor='white')\n",
    "ax.set(xlabel=r'$\\alpha$', ylabel=r'$\\lambda$', yscale='log')\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for i, label in enumerate(ax.get_xticklabels()):\n",
    "    if i > 0:\n",
    "        ax.axvline(i - 0.5, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "# fig.savefig(os.path.join(savedir, 'polyps_lambda.pdf'), pad_inches=0, bbox_inches='tight')\n",
    "# fig.savefig(os.path.join(savedir, 'polyps_lambda.png'), pad_inches=0, bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same plot, but using seaborn objects. Unfortunately, Seaborn objects does not support boxplots.\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4), tight_layout=True)\n",
    "\n",
    "(\n",
    "    so.Plot(df[df['alpha'] <= 0.1], x=\"alpha\", y=\"value\", color='model')\n",
    "    .add(so.Dot(), so.Dodge(), so.Jitter(.3))\n",
    "    .scale(x=so.Continuous().tick(at=df['alpha'].unique().tolist()))\n",
    "    .facet('metric')\n",
    "    .share(y=False)\n",
    "    .on(fig).plot()\n",
    ")\n",
    "\n",
    "axs = fig.get_axes()\n",
    "axs[2].set(yscale='log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images\n",
    "\n",
    "Based on the best learning rates above, run\n",
    "\n",
    "```bash\n",
    "python run_polyp.py savepreds -s <seed> --tag <tag> --ckpt-path <path/to/ckpt.pt> --device cuda\n",
    "```\n",
    "\n",
    "to generate PNG images of predictions on the test set. See the README for examples.\n",
    "\n",
    "Here, we only use seed 0 for generating images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "import PIL.Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "IMG_SIZE = 352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(5)\n",
    "all_test_img_names = sorted(os.listdir('out/polyps/preds_pretrained_s0'))\n",
    "test_img_names = sorted(random.sample(all_test_img_names, k=10))\n",
    "test_img_names = [os.path.splitext(n)[0] for n in test_img_names]\n",
    "pprint(test_img_names)\n",
    "\n",
    "test_img_names = [\n",
    "    'CVC-ClinicDB_400',\n",
    "    'CVC-ColonDB_101',\n",
    "    'CVC-ColonDB_291',\n",
    "    'CVC-ColonDB_296',\n",
    "    'CVC-ColonDB_46',\n",
    "    'ETIS-LaribPolypDB_171',\n",
    "    'ETIS-LaribPolypDB_43',\n",
    "    'Kvasir-SEG_cju45v0pungu40871acnwtmu5',\n",
    "    'Kvasir-SEG_cju88cddensj00987788yotmg',\n",
    "    'Kvasir-SEG_cju88vx2uoocy075531lc63n3'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_sr = df[(df['seed'] == 0) & (df['metric'] == 'lambda')].set_index(['alpha', 'model'])['value']\n",
    "lambdas_sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "    transforms.PILToTensor()\n",
    "])\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "def fn_fp_to_color(gt: torch.Tensor, pred: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert ground truth and prediction masks to a color image highlighting\n",
    "    false negatives and false positives.\n",
    "\n",
    "    Args:\n",
    "        gt: ground truth mask, shape [H, W], dtype torch.bool\n",
    "        pred: predicted mask, shape [H, W], dtype torch.bool\n",
    "    \"\"\"\n",
    "    assert gt.shape == pred.shape\n",
    "    assert gt.dtype == torch.bool\n",
    "    assert pred.dtype == torch.bool\n",
    "\n",
    "    h, w = gt.shape\n",
    "    out = torch.zeros((3, h, w), dtype=torch.uint8)\n",
    "\n",
    "    # correct: white or black\n",
    "    out[:, (gt == 1) & (pred == 1)] = 255\n",
    "\n",
    "    # false negative: red\n",
    "    out[0, (gt == 1) & (pred == 0)] = 255\n",
    "\n",
    "    # false positive: teal\n",
    "    out[1:, (gt == 0) & (pred == 1)] = 128\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def process_img(img_name: str):\n",
    "    img_tensors: dict[Any, torch.Tensor] = {}\n",
    "\n",
    "    ds, img_id = img_name.split('_')\n",
    "    img_path = os.path.join('polyps/data/test', ds, 'images', f'{img_id}.png')\n",
    "    gt_path = os.path.join('polyps/data/test', ds, 'masks', f'{img_id}.png')\n",
    "\n",
    "    pil_image = PIL.Image.open(img_path)\n",
    "    pil_gt = PIL.Image.open(gt_path)\n",
    "    assert pil_image.mode == 'RGB'\n",
    "    assert pil_gt.mode == '1'\n",
    "\n",
    "    image = img_transform(pil_image)  # [3, H, W], torch.float32\n",
    "    gt = gt_transform(pil_gt)  # [1, H, W], torch.bool\n",
    "    assert isinstance(image, torch.Tensor)\n",
    "    assert isinstance(gt, torch.Tensor)\n",
    "\n",
    "    img_tensors['image'] = image\n",
    "    img_tensors['gt'] = gt\n",
    "\n",
    "    # close file handles\n",
    "    pil_image.close()\n",
    "    pil_gt.close()\n",
    "\n",
    "    # pretrained predictions\n",
    "    pred_path = os.path.join('out/polyps/preds_pretrained_s0_raw', f'{img_name}.npy')\n",
    "    pretrained_pred_raw = torch.from_numpy(np.load(pred_path)[0])  # [H, W], torch.float32\n",
    "\n",
    "    # cross-entropy predictions\n",
    "    pred_path = os.path.join('out/polyps/preds_trainbase_s0_raw', f'{img_name}.npy')\n",
    "    trainbase_pred_raw = torch.from_numpy(np.load(pred_path)[0])  # [H, W], torch.float32\n",
    "\n",
    "    for alpha in [0.01, 0.05, 0.1]:\n",
    "        lam = lambdas_sr.loc[(alpha, 'post-hoc CRC')]\n",
    "        pretrained_pred = (pretrained_pred_raw >= lam)\n",
    "\n",
    "        lam = lambdas_sr.loc[(alpha, 'cross-entropy')]\n",
    "        trainbase_pred = (trainbase_pred_raw >= lam)\n",
    "\n",
    "        lam = lambdas_sr.loc[(alpha, 'conformal risk training')]\n",
    "        pred_path = os.path.join(f'out/polyps/preds_e2ecrc_a{alpha:.2f}_s0_raw', f'{img_name}.npy')\n",
    "        e2e_pred = (torch.from_numpy(np.load(pred_path)[0]) >= lam)\n",
    "\n",
    "        img_tensors[(alpha, 'pretrained')] = fn_fp_to_color(gt[0], pretrained_pred)\n",
    "        img_tensors[(alpha, 'trainbase')] = fn_fp_to_color(gt[0], trainbase_pred)\n",
    "        img_tensors[(alpha, 'e2e')] = fn_fp_to_color(gt[0], e2e_pred)\n",
    "\n",
    "    alpha = 0.20\n",
    "    lam = lambdas_sr.loc[(alpha, 'post-hoc CRC')]\n",
    "    pretrained_pred = (pretrained_pred_raw >= lam)\n",
    "    img_tensors[(alpha, 'pretrained')] = fn_fp_to_color(gt[0], pretrained_pred)\n",
    "\n",
    "    return img_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot everything in 1 figure\n",
    "\n",
    "num_images = 8  # len(test_img_names)\n",
    "spacing = 0.05\n",
    "fig, axs = plt.subplots(10, num_images, figsize=((1+spacing) * num_images + 1.5, (1+spacing)*10), gridspec_kw=dict(wspace=0, hspace=0), tight_layout=True)\n",
    "\n",
    "for c, img_name in enumerate(test_img_names[:num_images]):\n",
    "    img_tensors = process_img(img_name)\n",
    "\n",
    "    # plot image\n",
    "    ax = axs[0, c]\n",
    "    ax.imshow(img_tensors['image'].permute(1, 2, 0).numpy())\n",
    "\n",
    "    for r, alpha in enumerate([0.01, 0.05, 0.1]):\n",
    "        ax = axs[3*r + 1, c]\n",
    "        ax.imshow(img_tensors[(alpha, 'pretrained')].permute(1, 2, 0).numpy())\n",
    "\n",
    "        ax = axs[3*r + 2, c]\n",
    "        ax.imshow(img_tensors[(alpha, 'trainbase')].permute(1, 2, 0).numpy())\n",
    "\n",
    "        ax = axs[3*r + 3, c]\n",
    "        ax.imshow(img_tensors[(alpha, 'e2e')].permute(1, 2, 0).numpy())\n",
    "\n",
    "    # plot alpha=0.2 pretrained\n",
    "    # ax = axs[-1, c]\n",
    "    # ax.imshow(img_tensors[(0.2, 'pretrained')].permute(1, 2, 0).numpy())\n",
    "\n",
    "for ax in axs.flatten():\n",
    "    ax.set(xticks=[], yticks=[])\n",
    "\n",
    "axs[0, 0].set_ylabel('image')\n",
    "axs[1, 0].set_ylabel(r'$\\alpha=0.01$,' '\\npost-hoc CRC')\n",
    "axs[2, 0].set_ylabel(r'$\\alpha=0.01$,' '\\ncross-entropy')\n",
    "axs[3, 0].set_ylabel(r'$\\alpha=0.01$,' '\\nconformal risk training')\n",
    "axs[4, 0].set_ylabel(r'$\\alpha=0.05$,' '\\npost-hoc CRC')\n",
    "axs[5, 0].set_ylabel(r'$\\alpha=0.05$,' '\\ncross-entropy')\n",
    "axs[6, 0].set_ylabel(r'$\\alpha=0.05$,' '\\nconformal risk training')\n",
    "axs[7, 0].set_ylabel(r'$\\alpha=0.10$,' '\\npost-hoc CRC')\n",
    "axs[8, 0].set_ylabel(r'$\\alpha=0.10$,' '\\ncross-entropy')\n",
    "axs[9, 0].set_ylabel(r'$\\alpha=0.10$,' '\\nconformal risk training')\n",
    "\n",
    "for ax in axs[:, 0]:\n",
    "    ax.yaxis.label.set(rotation='horizontal', ha='right', va='center')\n",
    "\n",
    "fig.savefig(os.path.join(savedir, 'polyps_predictions_allinone.png'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "fig.savefig(os.path.join(savedir, 'polyps_predictions_allinone.pdf'), dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 4 separate figures\n",
    "\n",
    "num_images = 8  # len(test_img_names)\n",
    "spacing = 0.05\n",
    "\n",
    "# input images\n",
    "figs = {}\n",
    "axs = {}\n",
    "figs['input'], axs['input'] = plt.subplots(1, num_images, figsize=((1+spacing) * num_images, 1+spacing))\n",
    "for alpha in [0.01, 0.05, 0.1]:\n",
    "    figs[alpha], axs[alpha] = plt.subplots(3, num_images, figsize=((1+spacing) * num_images, (1+spacing)*3))\n",
    "\n",
    "\n",
    "for c, img_name in enumerate(test_img_names[:num_images]):\n",
    "    img_tensors = process_img(img_name)\n",
    "\n",
    "    # plot image\n",
    "    ax = axs['input'][c]\n",
    "    ax.imshow(img_tensors['image'].permute(1, 2, 0).numpy())\n",
    "\n",
    "    for r, alpha in enumerate([0.01, 0.05, 0.1]):\n",
    "        ax = axs[alpha][0, c]\n",
    "        ax.imshow(img_tensors[(alpha, 'pretrained')].permute(1, 2, 0).numpy())\n",
    "\n",
    "        ax = axs[alpha][1, c]\n",
    "        ax.imshow(img_tensors[(alpha, 'trainbase')].permute(1, 2, 0).numpy())\n",
    "\n",
    "        ax = axs[alpha][2, c]\n",
    "        ax.imshow(img_tensors[(alpha, 'e2e')].permute(1, 2, 0).numpy())\n",
    "\n",
    "    # plot alpha=0.2 pretrained\n",
    "    # ax = axs[-1, c]\n",
    "    # ax.imshow(img_tensors[(0.2, 'pretrained')].permute(1, 2, 0).numpy())\n",
    "\n",
    "for ax_dict in axs.values():\n",
    "    for ax in ax_dict.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "for fig in figs.values():\n",
    "    fig.tight_layout(pad=0, w_pad=0.5, h_pad=0.5)\n",
    "\n",
    "figs['input'].savefig(os.path.join(savedir, 'polyps_predictions_input.png'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "figs['input'].savefig(os.path.join(savedir, 'polyps_predictions_input.pdf'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "for alpha in [0.01, 0.05, 0.1]:\n",
    "    figs[alpha].savefig(os.path.join(savedir, f'polyps_predictions_a{alpha:.2f}.png'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "    figs[alpha].savefig(os.path.join(savedir, f'polyps_predictions_a{alpha:.2f}.pdf'), dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 10 separate figures\n",
    "\n",
    "num_images = 8  # len(test_img_names)\n",
    "spacing = 0.05\n",
    "\n",
    "# input images\n",
    "figs = {}\n",
    "axs = {}\n",
    "figs['input'], axs['input'] = plt.subplots(1, num_images, figsize=((1+spacing) * num_images, 1+spacing))\n",
    "for alpha in [0.01, 0.05, 0.1]:\n",
    "    for model in ['pretrained', 'trainbase', 'e2e']:\n",
    "        figs[(alpha, model)], axs[(alpha, model)] = plt.subplots(1, num_images, figsize=((1+spacing) * num_images, 1+spacing))\n",
    "\n",
    "\n",
    "for c, img_name in enumerate(test_img_names[:num_images]):\n",
    "    img_tensors = process_img(img_name)\n",
    "\n",
    "    # plot image\n",
    "    ax = axs['input'][c]\n",
    "    ax.imshow(img_tensors['image'].permute(1, 2, 0).numpy())\n",
    "\n",
    "    for alpha in [0.01, 0.05, 0.1]:\n",
    "        for model in ['pretrained', 'trainbase', 'e2e']:\n",
    "            ax = axs[(alpha, model)][c]\n",
    "            ax.imshow(img_tensors[(alpha, model)].permute(1, 2, 0).numpy())\n",
    "\n",
    "for ax_dict in axs.values():\n",
    "    for ax in ax_dict.flatten():\n",
    "        ax.axis('off')\n",
    "\n",
    "for fig in figs.values():\n",
    "    fig.tight_layout(pad=0, w_pad=0.5, h_pad=0.5)\n",
    "\n",
    "figs['input'].savefig(os.path.join(savedir, 'polyps_predictions_input.png'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "figs['input'].savefig(os.path.join(savedir, 'polyps_predictions_input.pdf'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "for alpha in [0.01, 0.05, 0.1]:\n",
    "    for model in ['pretrained', 'trainbase', 'e2e']:\n",
    "        figs[(alpha, model)].savefig(os.path.join(savedir, f'polyps_predictions_a{alpha:.2f}_{model}.png'), dpi=300, bbox_inches='tight', pad_inches=0)\n",
    "        figs[(alpha, model)].savefig(os.path.join(savedir, f'polyps_predictions_a{alpha:.2f}_{model}.pdf'), dpi=300, bbox_inches='tight', pad_inches=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2ecrc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
