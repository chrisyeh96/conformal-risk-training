{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d04727",
   "metadata": {},
   "source": [
    "This notebook examines the effect of varying the size of the calibration dataset on the mean task loss and financial CVaR tail risk on the battery storage problem, for the pre-trained MLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a2198",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48752318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable, Mapping\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.mlp import MLP\n",
    "from storage.data import get_tensors, get_train_calib_split\n",
    "from storage.problems import (\n",
    "    StorageConstants, StorageProblemNonRobust, StorageProblemLambda)\n",
    "from run_storage import cvar, get_zs\n",
    "\n",
    "Device = str | torch.device\n",
    "\n",
    "INPUT_DIM = 101  # including future_temp\n",
    "Y_DIM = 24\n",
    "MAX_PRETRAIN_EPOCHS = 500\n",
    "MAX_FINETUNE_EPOCHS = 100\n",
    "BATCH_SIZE = 400\n",
    "PSEUDOCAILB_SIZE = 200\n",
    "SEEDS = range(10)\n",
    "LOG_PRICES = False\n",
    "LABEL_NOISE = 20\n",
    "\n",
    "STORAGE_CONSTS = [\n",
    "    StorageConstants(lam=0.1, eps=.05),\n",
    "]\n",
    "\n",
    "out_dir = 'out/storage_mlp_shuffle/'\n",
    "\n",
    "savedir = 'analysis/plots'\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a811d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lams(\n",
    "    tensors_dict: Mapping[str, Tensor], model: MLP, prob: StorageProblemNonRobust,\n",
    "    device: Device, alphas: Iterable[float], deltas: Iterable[float]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns:\n",
    "        alpha, delta, lambda, t, task loss, CVaR\n",
    "    \"\"\"\n",
    "    X_train = tensors_dict['X_train'].to(device, non_blocking=True)\n",
    "    X_val = tensors_dict['X_calib'].to(device, non_blocking=True)\n",
    "    Y_train_np = tensors_dict['Y_train'].cpu().numpy()\n",
    "    Y_val_np = tensors_dict['Y_calib'].cpu().numpy()\n",
    "\n",
    "    # get decision variables\n",
    "    with torch.no_grad():\n",
    "        model.eval().to(device)\n",
    "        pred_np_train = model(X_train).cpu().numpy()\n",
    "        pred_np_val = model(X_val).cpu().numpy()\n",
    "    z_in_train, z_out_train, z_net_train = get_zs(prob, pred_np_train)\n",
    "    z_in_val, z_out_val, z_net_val = get_zs(prob, pred_np_val)\n",
    "\n",
    "    max_lambda_prob_var_t = StorageProblemLambda(\n",
    "        T=Y_DIM, const=prob.const,\n",
    "        y=Y_train_np, y_mean=prob.y_mean, y_std=prob.y_std,\n",
    "        z_in=z_in_train, z_out=z_out_train, z_net=z_net_train, quad=False, t_fixed=False)\n",
    "\n",
    "    max_lambda_prob_fixed_t = StorageProblemLambda(\n",
    "        T=Y_DIM, const=prob.const,\n",
    "        y=Y_val_np, y_mean=prob.y_mean, y_std=prob.y_std,\n",
    "        z_in=z_in_val, z_out=z_out_val, z_net=z_net_val, quad=False, t_fixed=True)\n",
    "\n",
    "    rows = []\n",
    "    for alpha, delta in itertools.product(alphas, deltas):\n",
    "        max_lambda_prob_var_t.solve(alpha, delta)\n",
    "        assert max_lambda_prob_var_t.t.value is not None\n",
    "        t = max_lambda_prob_var_t.t.value.item()\n",
    "\n",
    "        λ = max_lambda_prob_fixed_t.solve(alpha, delta, t=t)\n",
    "        if λ == 0.:\n",
    "            val_task_loss = 0.\n",
    "            val_cvar = 0.\n",
    "        else:\n",
    "            task_losses = prob.task_loss(\n",
    "                z_in_val * λ, z_out_val * λ, z_net_val * λ,\n",
    "                y=Y_val_np, is_standardized=True)\n",
    "            financial_losses = prob.financial_loss(\n",
    "                z_in_val * λ, z_out_val * λ, y=Y_val_np, is_standardized=True)\n",
    "            assert isinstance(task_losses, np.ndarray)\n",
    "            assert isinstance(financial_losses, np.ndarray)\n",
    "            val_task_loss = np.mean(task_losses).item()\n",
    "            val_cvar = cvar(financial_losses, q=delta)\n",
    "\n",
    "        rows.append({\n",
    "            'alpha': alpha, 'delta': delta,\n",
    "            'lambda': λ,\n",
    "            't': t,\n",
    "            'task loss': val_task_loss,\n",
    "            'CVaR': val_cvar\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).set_index(['alpha', 'delta'])\n",
    "\n",
    "\n",
    "def crc(\n",
    "    shuffle: bool, future_temp: bool, label_noise: float, const: StorageConstants,\n",
    "    alphas: Iterable[float], deltas: Iterable[float], seed: int, saved_ckpt_fmt: str,\n",
    "    device: Device\n",
    ") -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Post-hoc CRC. Always call this function within a torch.no_grad() context.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts, one per (alpha, delta) pair, with keys:\n",
    "            seed, alpha, delta, lambda, task loss, cvar\n",
    "    \"\"\"\n",
    "    tensors, y_info = get_tensors(\n",
    "        shuffle=shuffle, log_prices=LOG_PRICES, future_temp=future_temp,\n",
    "        label_noise=label_noise)\n",
    "    assert isinstance(y_info, tuple)\n",
    "    y_mean, y_std = y_info\n",
    "    tensors_cv, _ = get_train_calib_split(tensors, seed=seed)\n",
    "\n",
    "    prob = StorageProblemNonRobust(T=Y_DIM, y_mean=y_mean, y_std=y_std, const=const)\n",
    "\n",
    "    # load the model\n",
    "    model = MLP(input_dim=tensors['X_test'].shape[1], y_dim=Y_DIM)\n",
    "    saved_ckpt_path = saved_ckpt_fmt.format(seed=seed)\n",
    "    model.load_state_dict(torch.load(saved_ckpt_path, weights_only=True))\n",
    "    model.eval().to(device)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    calib_sizes = [None, 300, 100]\n",
    "    for calib_size in calib_sizes:\n",
    "        new_tensors_cv = tensors_cv.copy()\n",
    "        if calib_size is not None:\n",
    "            new_tensors_cv['X_calib'] = tensors_cv['X_calib'][:calib_size]\n",
    "            new_tensors_cv['Y_calib'] = tensors_cv['Y_calib'][:calib_size]\n",
    "        else:\n",
    "            calib_size = len(new_tensors_cv['Y_calib'])\n",
    "\n",
    "        calib_df = get_lams(\n",
    "            tensors_dict=new_tensors_cv, model=model,\n",
    "            prob=prob, device=device, alphas=alphas, deltas=deltas)\n",
    "\n",
    "        # use lambdas on test set\n",
    "        with torch.no_grad():\n",
    "            model.eval().to(device)\n",
    "            pred_np = model(tensors['X_test'].to(device)).cpu().numpy()  # type: ignore\n",
    "        z_in, z_out, z_net = get_zs(prob, preds=pred_np)\n",
    "        y_test = tensors['Y_test'].cpu().numpy()  # type: ignore\n",
    "        for (alpha, delta) in calib_df.index:\n",
    "            λ = calib_df.loc[(alpha, delta), 'lambda']\n",
    "            t = calib_df.loc[(alpha, delta), 't']\n",
    "            task_losses = prob.task_loss(z_in * λ, z_out * λ, z_net * λ, y=y_test, is_standardized=True)\n",
    "            financial_losses = prob.financial_loss(z_in * λ, z_out * λ, y=y_test, is_standardized=True)\n",
    "            assert isinstance(task_losses, np.ndarray)\n",
    "            assert isinstance(financial_losses, np.ndarray)\n",
    "            rows.append({\n",
    "                'seed': seed, 'alpha': alpha, 'delta': delta, 'calib_size': calib_size,\n",
    "                't': t, 'lambda': λ,\n",
    "                'task loss': np.mean(task_losses).item(),\n",
    "                'cvar': cvar(financial_losses, q=delta)\n",
    "            })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows: list[dict[str, float]] = []\n",
    "saved_ckpt_fmt = os.path.join(out_dir, 'mlp_s{seed}.pt')\n",
    "for s in tqdm(SEEDS):\n",
    "    crc_results = crc(\n",
    "        seed=s, shuffle=True, future_temp=False,\n",
    "        label_noise=LABEL_NOISE, const=STORAGE_CONSTS[0],\n",
    "        alphas=[2, 5, 10], deltas=[.9, .95, .99],\n",
    "        saved_ckpt_fmt=saved_ckpt_fmt, device='cuda:0')\n",
    "    all_rows.extend(crc_results)\n",
    "\n",
    "# save results to file\n",
    "df = pd.DataFrame(all_rows)\n",
    "\n",
    "df.to_csv(os.path.join(out_dir, 'crc_vary_calibsize.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36decb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(out_dir, 'crc_vary_calibsize.csv'))\n",
    "df = df.rename(columns={'delta': 'δ', 'alpha': 'α', 'lambda': 'λ'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['α', 'δ', 'calib_size', 'seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12382bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    df[['task loss', 'cvar']]\n",
    "    .groupby(['α', 'δ', 'calib_size'])\n",
    "    .agg(['mean', 'std'])\n",
    ")\n",
    "\n",
    "def mean_std(row: pd.Series, mean_fmt: str = '{:.2g}', std_fmt: str = '{:.2g}') -> str:\n",
    "    mean = row['mean']\n",
    "    std = row['std']\n",
    "    if np.isnan(mean):\n",
    "        assert np.isnan(std)\n",
    "        return \"nan\"\n",
    "    return f'{mean_fmt.format(mean)} ± {std_fmt.format(std)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 100):\n",
    "    display(pd.DataFrame({\n",
    "        'task loss': summary['task loss'].apply(mean_std, axis=1),\n",
    "        'cvar': summary['cvar'].apply(mean_std, axis=1)\n",
    "    }))\n",
    "\n",
    "    fmt = '{:.1f}'\n",
    "    kwargs = {'mean_fmt': fmt, 'std_fmt': fmt}\n",
    "\n",
    "    print(\n",
    "        summary['task loss'].apply(mean_std, axis=1, **kwargs).unstack(['α', 'δ']).to_latex()\n",
    "    )\n",
    "    print(\n",
    "        summary['cvar'].apply(mean_std, axis=1, **kwargs).unstack(['α', 'δ']).to_latex()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b62dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df[['task loss', 'cvar']]\n",
    "df_long.columns.name = 'metric'\n",
    "df_long = df_long.stack().to_frame(name='value')\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6efd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ac0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=df_long, x='α', y='value', hue='calib_size',\n",
    "    col='δ', row='metric',\n",
    "    sharey=False, kind='box', height=2.8, aspect=1.4, \n",
    "    width=0.5\n",
    ")\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for ax in g.axes.flatten():\n",
    "    ticks = ax.get_xticks()\n",
    "    for i in range(1, len(ticks)):\n",
    "        ax.axvline((ticks[i] + ticks[i-1])/2, color=\"gray\", linestyle=\"--\", linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = r'CVaR${}^\\delta[L(\\theta, \\lambda)]$'\n",
    "legend_title = 'calibration\\nset size'\n",
    "cvar_df = df_long.unstack('metric')[('value', 'cvar')].rename(ylabel).to_frame()\n",
    "cvar_df.index = cvar_df.index.set_names(['α', 'δ', legend_title, 'seed'])\n",
    "\n",
    "g_cvar = sns.catplot(\n",
    "    data=cvar_df, x='α', y=ylabel, hue=legend_title, col='δ',\n",
    "    sharey=True, kind='box', height=2.8,\n",
    "    palette='Set2',\n",
    ")\n",
    "\n",
    "# center the legend title\n",
    "assert g_cvar.legend is not None\n",
    "g_cvar.legend.get_title().set_multialignment('center')\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for ax in g_cvar.axes.flatten():\n",
    "    ticks = ax.get_xticks()\n",
    "    for i in range(1, len(ticks)):\n",
    "        ax.axvline((ticks[i] + ticks[i-1])/2, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "g_cvar.tight_layout(pad=0, w_pad=1.08)\n",
    "g_cvar.figure.savefig(os.path.join(savedir, 'storage_cvar_by_calibsize.pdf'), pad_inches=0)\n",
    "g_cvar.figure.savefig(os.path.join(savedir, 'storage_cvar_by_calibsize.png'), pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = 'task loss'\n",
    "legend_title = 'calibration\\nset size'\n",
    "taskloss_df = df_long.unstack('metric')[('value', 'task loss')].rename(ylabel).to_frame()\n",
    "taskloss_df.index = taskloss_df.index.set_names(['α', 'δ', legend_title, 'seed'])\n",
    "\n",
    "g_taskloss = sns.catplot(\n",
    "    data=taskloss_df, x='α', y=ylabel, hue=legend_title, col='δ',\n",
    "    sharey=True, kind='box', height=2.8,\n",
    "    palette='Set2',\n",
    ")\n",
    "\n",
    "# center the legend title\n",
    "assert g_taskloss.legend is not None\n",
    "g_taskloss.legend.get_title().set_multialignment('center')\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for ax in g_taskloss.axes.flatten():\n",
    "    ticks = ax.get_xticks()\n",
    "    for i in range(1, len(ticks)):\n",
    "        ax.axvline((ticks[i] + ticks[i-1])/2, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "g_taskloss.tight_layout(pad=0, w_pad=1.08)\n",
    "g_taskloss.figure.savefig(os.path.join(savedir, 'storage_taskloss_by_calibsize.pdf'), pad_inches=0)\n",
    "g_taskloss.figure.savefig(os.path.join(savedir, 'storage_taskloss_by_calibsize.png'), pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn.objects as so\n",
    "\n",
    "(\n",
    "    so.Plot(df_long, x='calib_size', y='value', color='α')\n",
    "    .add(so.Bar(), so.Agg(), so.Dodge())\n",
    "    .facet(col='δ', row='metric')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2ecrc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
