{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d04727",
   "metadata": {},
   "source": [
    "This notebook examines the effect of varying the size of the calibration dataset on the mean task loss and financial CVaR tail risk on the battery storage problem, for the pre-trained MLP models.\n",
    "\n",
    "The following files are assumed to exist. These files are created by running the commands listed in the README file under the heading, \"Example: Battery storage CVaR control.\"\n",
    "\n",
    "```\n",
    "out/\n",
    "  storage_mlp_shuffle/\n",
    "    mlp_s{seed}.pt\n",
    "```\n",
    "\n",
    "This notebook creates the following files:\n",
    "\n",
    "```\n",
    "analysis/\n",
    "  plots/\n",
    "    storage_cvar_by_calibsize.pdf/.png\n",
    "    storage_taskloss_by_calibsize.pdf/.png\n",
    "out/\n",
    "  storage_mlp_shuffle/\n",
    "    crc_vary_calibsize.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5a2198",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48752318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Iterable, Mapping\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from models.mlp import MLP\n",
    "from storage.data import get_tensors, get_train_calib_split\n",
    "from storage.problems import (\n",
    "    StorageConstants, StorageProblemNonRobust, StorageProblemLambda)\n",
    "from run_storage import cvar, get_zs\n",
    "\n",
    "Device = str | torch.device\n",
    "\n",
    "INPUT_DIM = 101  # including future_temp\n",
    "Y_DIM = 24\n",
    "MAX_PRETRAIN_EPOCHS = 500\n",
    "MAX_FINETUNE_EPOCHS = 100\n",
    "BATCH_SIZE = 400\n",
    "PSEUDOCAILB_SIZE = 200\n",
    "SEEDS = range(10)\n",
    "LOG_PRICES = False\n",
    "LABEL_NOISE = 20\n",
    "\n",
    "STORAGE_CONSTS = [\n",
    "    StorageConstants(lam=0.1, eps=.05),\n",
    "]\n",
    "\n",
    "out_dir = 'out/storage_mlp_shuffle/'\n",
    "\n",
    "savedir = 'analysis/plots'\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a811d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lams(\n",
    "    tensors_dict: Mapping[str, Tensor], model: MLP, prob: StorageProblemNonRobust,\n",
    "    device: Device, alphas: Iterable[float], deltas: Iterable[float]\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns:\n",
    "        alpha, delta, lambda, t, task loss, CVaR\n",
    "    \"\"\"\n",
    "    X_train = tensors_dict['X_train'].to(device, non_blocking=True)\n",
    "    X_val = tensors_dict['X_calib'].to(device, non_blocking=True)\n",
    "    Y_train_np = tensors_dict['Y_train'].cpu().numpy()\n",
    "    Y_val_np = tensors_dict['Y_calib'].cpu().numpy()\n",
    "\n",
    "    # get decision variables\n",
    "    with torch.no_grad():\n",
    "        model.eval().to(device)\n",
    "        pred_np_train = model(X_train).cpu().numpy()\n",
    "        pred_np_val = model(X_val).cpu().numpy()\n",
    "    z_in_train, z_out_train, z_net_train = get_zs(prob, pred_np_train)\n",
    "    z_in_val, z_out_val, z_net_val = get_zs(prob, pred_np_val)\n",
    "\n",
    "    max_lambda_prob_var_t = StorageProblemLambda(\n",
    "        T=Y_DIM, const=prob.const,\n",
    "        y=Y_train_np, y_mean=prob.y_mean, y_std=prob.y_std,\n",
    "        z_in=z_in_train, z_out=z_out_train, z_net=z_net_train, quad=False, t_fixed=False)\n",
    "\n",
    "    max_lambda_prob_fixed_t = StorageProblemLambda(\n",
    "        T=Y_DIM, const=prob.const,\n",
    "        y=Y_val_np, y_mean=prob.y_mean, y_std=prob.y_std,\n",
    "        z_in=z_in_val, z_out=z_out_val, z_net=z_net_val, quad=False, t_fixed=True)\n",
    "\n",
    "    rows = []\n",
    "    for alpha, delta in itertools.product(alphas, deltas):\n",
    "        max_lambda_prob_var_t.solve(alpha, delta)\n",
    "        assert max_lambda_prob_var_t.t.value is not None\n",
    "        t = max_lambda_prob_var_t.t.value.item()\n",
    "\n",
    "        λ = max_lambda_prob_fixed_t.solve(alpha, delta, t=t)\n",
    "        if λ == 0.:\n",
    "            val_task_loss = 0.\n",
    "            val_cvar = 0.\n",
    "        else:\n",
    "            task_losses = prob.task_loss(\n",
    "                z_in_val * λ, z_out_val * λ, z_net_val * λ,\n",
    "                y=Y_val_np, is_standardized=True)\n",
    "            financial_losses = prob.financial_loss(\n",
    "                z_in_val * λ, z_out_val * λ, y=Y_val_np, is_standardized=True)\n",
    "            assert isinstance(task_losses, np.ndarray)\n",
    "            assert isinstance(financial_losses, np.ndarray)\n",
    "            val_task_loss = np.mean(task_losses).item()\n",
    "            val_cvar = cvar(financial_losses, q=delta)\n",
    "\n",
    "        rows.append({\n",
    "            'alpha': alpha, 'delta': delta,\n",
    "            'lambda': λ,\n",
    "            't': t,\n",
    "            'task loss': val_task_loss,\n",
    "            'CVaR': val_cvar\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(rows).set_index(['alpha', 'delta'])\n",
    "\n",
    "\n",
    "def crc(\n",
    "    shuffle: bool, future_temp: bool, label_noise: float, const: StorageConstants,\n",
    "    alphas: Iterable[float], deltas: Iterable[float], seed: int, saved_ckpt_fmt: str,\n",
    "    device: Device\n",
    ") -> list[dict[str, float]]:\n",
    "    \"\"\"\n",
    "    Post-hoc CRC. Always call this function within a torch.no_grad() context.\n",
    "\n",
    "    Returns:\n",
    "        list of dicts, one per (alpha, delta) pair, with keys:\n",
    "            seed, alpha, delta, lambda, task loss, cvar\n",
    "    \"\"\"\n",
    "    tensors, y_info = get_tensors(\n",
    "        shuffle=shuffle, log_prices=LOG_PRICES, future_temp=future_temp,\n",
    "        label_noise=label_noise)\n",
    "    assert isinstance(y_info, tuple)\n",
    "    y_mean, y_std = y_info\n",
    "    tensors_cv, _ = get_train_calib_split(tensors, seed=seed)\n",
    "\n",
    "    prob = StorageProblemNonRobust(T=Y_DIM, y_mean=y_mean, y_std=y_std, const=const)\n",
    "\n",
    "    # load the model\n",
    "    model = MLP(input_dim=tensors['X_test'].shape[1], y_dim=Y_DIM)\n",
    "    saved_ckpt_path = saved_ckpt_fmt.format(seed=seed)\n",
    "    model.load_state_dict(torch.load(saved_ckpt_path, weights_only=True))\n",
    "    model.eval().to(device)\n",
    "\n",
    "    rows = []\n",
    "\n",
    "    calib_sizes = [None, 300, 100]\n",
    "    for calib_size in calib_sizes:\n",
    "        new_tensors_cv = tensors_cv.copy()\n",
    "        if calib_size is not None:\n",
    "            new_tensors_cv['X_calib'] = tensors_cv['X_calib'][:calib_size]\n",
    "            new_tensors_cv['Y_calib'] = tensors_cv['Y_calib'][:calib_size]\n",
    "        else:\n",
    "            calib_size = len(new_tensors_cv['Y_calib'])\n",
    "\n",
    "        calib_df = get_lams(\n",
    "            tensors_dict=new_tensors_cv, model=model,\n",
    "            prob=prob, device=device, alphas=alphas, deltas=deltas)\n",
    "\n",
    "        # use lambdas on test set\n",
    "        with torch.no_grad():\n",
    "            model.eval().to(device)\n",
    "            pred_np = model(tensors['X_test'].to(device)).cpu().numpy()  # type: ignore\n",
    "        z_in, z_out, z_net = get_zs(prob, preds=pred_np)\n",
    "        y_test = tensors['Y_test'].cpu().numpy()  # type: ignore\n",
    "        for (alpha, delta) in calib_df.index:\n",
    "            λ = calib_df.loc[(alpha, delta), 'lambda']\n",
    "            t = calib_df.loc[(alpha, delta), 't']\n",
    "            task_losses = prob.task_loss(z_in * λ, z_out * λ, z_net * λ, y=y_test, is_standardized=True)\n",
    "            financial_losses = prob.financial_loss(z_in * λ, z_out * λ, y=y_test, is_standardized=True)\n",
    "            assert isinstance(task_losses, np.ndarray)\n",
    "            assert isinstance(financial_losses, np.ndarray)\n",
    "            rows.append({\n",
    "                'seed': seed, 'alpha': alpha, 'delta': delta, 'calib_size': calib_size,\n",
    "                't': t, 'lambda': λ,\n",
    "                'task loss': np.mean(task_losses).item(),\n",
    "                'cvar': cvar(financial_losses, q=delta)\n",
    "            })\n",
    "\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rows: list[dict[str, float]] = []\n",
    "saved_ckpt_fmt = os.path.join(out_dir, 'mlp_s{seed}.pt')\n",
    "for s in tqdm(SEEDS):\n",
    "    crc_results = crc(\n",
    "        seed=s, shuffle=True, future_temp=False,\n",
    "        label_noise=LABEL_NOISE, const=STORAGE_CONSTS[0],\n",
    "        alphas=[2, 5, 10], deltas=[.9, .95, .99],\n",
    "        saved_ckpt_fmt=saved_ckpt_fmt, device='cuda:0')\n",
    "    all_rows.extend(crc_results)\n",
    "\n",
    "# save results to file\n",
    "df = pd.DataFrame(all_rows)\n",
    "\n",
    "df.to_csv(os.path.join(out_dir, 'crc_vary_calibsize.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36decb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(out_dir, 'crc_vary_calibsize.csv'))\n",
    "df = df.rename(columns={'delta': 'δ', 'alpha': 'α', 'lambda': 'λ'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7840ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(['α', 'δ', 'calib_size', 'seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12382bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = (\n",
    "    df[['task loss', 'cvar']]\n",
    "    .groupby(['α', 'δ', 'calib_size'])\n",
    "    .agg(['mean', 'std'])\n",
    ")\n",
    "\n",
    "def mean_std(row: pd.Series, mean_fmt: str = '{:.2g}', std_fmt: str = '{:.2g}') -> str:\n",
    "    mean = row['mean']\n",
    "    std = row['std']\n",
    "    if np.isnan(mean):\n",
    "        assert np.isnan(std)\n",
    "        return \"nan\"\n",
    "    return f'{mean_fmt.format(mean)} ± {std_fmt.format(std)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822f8aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', 100):\n",
    "    display(pd.DataFrame({\n",
    "        'task loss': summary['task loss'].apply(mean_std, axis=1),\n",
    "        'cvar': summary['cvar'].apply(mean_std, axis=1)\n",
    "    }))\n",
    "\n",
    "    fmt = '{:.1f}'\n",
    "    kwargs = {'mean_fmt': fmt, 'std_fmt': fmt}\n",
    "\n",
    "    print(\n",
    "        summary['task loss'].apply(mean_std, axis=1, **kwargs).unstack(['α', 'δ']).to_latex()\n",
    "    )\n",
    "    print(\n",
    "        summary['cvar'].apply(mean_std, axis=1, **kwargs).unstack(['α', 'δ']).to_latex()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b62dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long = df[['task loss', 'cvar']]\n",
    "df_long.columns.name = 'metric'\n",
    "df_long = df_long.stack().to_frame(name='value')\n",
    "df_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6efd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846ac0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(\n",
    "    data=df_long, x='α', y='value', hue='calib_size',\n",
    "    col='δ', row='metric',\n",
    "    sharey=False, kind='box', height=2.8, aspect=1.4, \n",
    "    width=0.5\n",
    ")\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for ax in g.axes.flatten():\n",
    "    ticks = ax.get_xticks()\n",
    "    for i in range(1, len(ticks)):\n",
    "        ax.axvline((ticks[i] + ticks[i-1])/2, color=\"gray\", linestyle=\"--\", linewidth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9bdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = r'CVaR${}^\\delta[L(\\theta, \\lambda)]$'\n",
    "legend_title = 'calibration\\nset size'\n",
    "cvar_df = df_long.unstack('metric')[('value', 'cvar')].rename(ylabel).to_frame()\n",
    "cvar_df.index = cvar_df.index.set_names(['α', 'δ', legend_title, 'seed'])\n",
    "\n",
    "g_cvar = sns.catplot(\n",
    "    data=cvar_df, x='α', y=ylabel, hue=legend_title, col='δ',\n",
    "    sharey=True, kind='box', height=2.8,\n",
    "    palette='Set2',\n",
    ")\n",
    "\n",
    "# center the legend title\n",
    "assert g_cvar.legend is not None\n",
    "g_cvar.legend.get_title().set_multialignment('center')\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for ax in g_cvar.axes.flatten():\n",
    "    ticks = ax.get_xticks()\n",
    "    for i in range(1, len(ticks)):\n",
    "        ax.axvline((ticks[i] + ticks[i-1])/2, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "g_cvar.tight_layout(pad=0, w_pad=1.08)\n",
    "g_cvar.figure.savefig(os.path.join(savedir, 'storage_cvar_by_calibsize.pdf'), pad_inches=0)\n",
    "g_cvar.figure.savefig(os.path.join(savedir, 'storage_cvar_by_calibsize.png'), pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1b4ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylabel = 'task loss'\n",
    "legend_title = 'calibration\\nset size'\n",
    "taskloss_df = df_long.unstack('metric')[('value', 'task loss')].rename(ylabel).to_frame()\n",
    "taskloss_df.index = taskloss_df.index.set_names(['α', 'δ', legend_title, 'seed'])\n",
    "\n",
    "g_taskloss = sns.catplot(\n",
    "    data=taskloss_df, x='α', y=ylabel, hue=legend_title, col='δ',\n",
    "    sharey=True, kind='box', height=2.8,\n",
    "    palette='Set2',\n",
    ")\n",
    "\n",
    "# center the legend title\n",
    "assert g_taskloss.legend is not None\n",
    "g_taskloss.legend.get_title().set_multialignment('center')\n",
    "\n",
    "# Iterate through the axes and add vertical lines\n",
    "for ax in g_taskloss.axes.flatten():\n",
    "    ticks = ax.get_xticks()\n",
    "    for i in range(1, len(ticks)):\n",
    "        ax.axvline((ticks[i] + ticks[i-1])/2, color=\"gray\", linestyle=\"--\", linewidth=1)\n",
    "\n",
    "g_taskloss.tight_layout(pad=0, w_pad=1.08)\n",
    "g_taskloss.figure.savefig(os.path.join(savedir, 'storage_taskloss_by_calibsize.pdf'), pad_inches=0)\n",
    "g_taskloss.figure.savefig(os.path.join(savedir, 'storage_taskloss_by_calibsize.png'), pad_inches=0, dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn.objects as so\n",
    "\n",
    "(\n",
    "    so.Plot(df_long, x='calib_size', y='value', color='α')\n",
    "    .add(so.Bar(), so.Agg(), so.Dodge())\n",
    "    .facet(col='δ', row='metric')\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "e2ecrc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
